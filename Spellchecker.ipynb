{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CT5120_CT5146 - Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oP1uun77cIh"
      },
      "source": [
        "# Spellchecker\n",
        "\n",
        "Spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIVCSJV-7kDs"
      },
      "source": [
        "## Task 1 (10 Marks)\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RznCZ1mw7mfk"
      },
      "source": [
        "lines = open(\"holbrook.txt\").readlines()\n",
        "data = []\n",
        "# Write your code here\n",
        "for line in lines:\n",
        "    indices = []\n",
        "    str1 = []\n",
        "    str2 = []\n",
        "    line = line.split()\n",
        "    for i in range(len(line)):\n",
        "        if \"|\" in line[i]:\n",
        "            indices.append(i)\n",
        "            str1.append(line[i].split(\"|\")[0])\n",
        "            str2.append(line[i].split(\"|\")[1])\n",
        "        else:\n",
        "            str1.append(line[i])\n",
        "            str2.append(line[i])\n",
        "    data.append({'original': str1, 'corrected': str2, 'indexes': indices})\n",
        "\n",
        "assert(data[2] == {\n",
        "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
        "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
        "    'indexes': [9]\n",
        "})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRSX4I0H7pSC"
      },
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt9aR2Gy7p1C"
      },
      "source": [
        "test = data[:100]\n",
        "train = data[100:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5JL7cH7sLK"
      },
      "source": [
        "## **Task 2** (10 Marks): \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ge0uHS-7uEK"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def unigram(word):\n",
        "    # Write your code here.\n",
        "    corrected_lines = []\n",
        "    for line in train:\n",
        "        corrected_lines.extend(line['corrected'])\n",
        "        for w in corrected_lines:\n",
        "          w.lower()\n",
        "    cnt = Counter(corrected_lines)\n",
        "    return cnt[word]\n",
        "    \n",
        "\n",
        "def prob(word):\n",
        "    count = unigram(word)\n",
        "    corrected_lines = []\n",
        "    for line in train:\n",
        "        corrected_lines.extend(line['corrected'])\n",
        "    total = len(corrected_lines)\n",
        "    prob = count/total\n",
        "    return prob\n",
        "\n",
        "# Test your code with the following\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8r8QYj78GPK"
      },
      "source": [
        "## **Task 3** (15 Marks): \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "outputId": "e8173a16-3a72-4e35-ad14-092456603d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm46Lbiz8K8M"
      },
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoilAmFW8PCb"
      },
      "source": [
        "def get_candidates(token):\n",
        "  tokens = []\n",
        "  for t in train:\n",
        "        tokens.extend(t[\"corrected\"])\n",
        "        #tokens.extend(t['original'])\n",
        "  token_set = set(tokens)\n",
        "  dmin = 100000\n",
        "  minDs = []\n",
        "  for t in token_set:\n",
        "      d = edit_distance(token, t)\n",
        "      if d == dmin:\n",
        "          minDs.append(t)\n",
        "      if d < dmin:\n",
        "          dmin = d\n",
        "          minDs = [t]\n",
        "  minDs.sort(reverse=True)\n",
        "  return minDs\n",
        "        \n",
        "# Test your code as follows\n",
        "assert get_candidates(\"minde\") == ['mine', 'mind']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGY-eCkN8TIM"
      },
      "source": [
        "## Task 4 (15 Marks):\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIGKE4_P8WGP"
      },
      "source": [
        "def correct(sentence):\n",
        "    tokens = []\n",
        "    for t in train:\n",
        "        tokens.extend(t[\"corrected\"])\n",
        "    token_set = set(tokens)\n",
        "    for word in sentence:\n",
        "        if word not in token_set:\n",
        "            candidates = get_candidates(word)\n",
        "            highest_prob = 0\n",
        "            for candidate in candidates:\n",
        "                probability = prob(word)\n",
        "                if probability >= highest_prob:\n",
        "                    highest_prob = probability\n",
        "                    sentence = [candidate if (x == word) else x for x in sentence]\n",
        "    return sentence\n",
        "\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG7jC6au8kka"
      },
      "source": [
        "## **Task 5** (10 Marks): \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXTQypR8mdR"
      },
      "source": [
        "def accuracy(test):\n",
        "    incorrect_count = 0\n",
        "    correct_count = 0\n",
        "    for sentence in test:\n",
        "      my_correction = correct(sentence['original'])\n",
        "      for i in range(len(my_correction)):\n",
        "        if my_correction[i] != sentence['corrected'][i]:\n",
        "          incorrect_count += 1\n",
        "          #print(my_correction[i])\n",
        "          #print(sentence['corrected'][i])\n",
        "        else:\n",
        "          correct_count +=1\n",
        "    total = correct_count + incorrect_count\n",
        "    accuracy = correct_count/total\n",
        "    return accuracy\n",
        "\n",
        "print(accuracy(test))\n",
        "# slow to run but returns 0.8148804251550045"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-r2JzD8_Zh"
      },
      "source": [
        "## **Task 6 (35 Marks):**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTWbpd01d4w-"
      },
      "source": [
        "Some modifications which could be implemented to improve the accuracy of the algorithm are to increase the unigram probability to check to bigrams or trigrams. Increased values of n in n-gram probabilty checks take into consideration the context in which a token exists. Unigram probability ignores the liklihood of any words coexistance and focuses soley on the existance of individual words in the text. It is recommended that different values on n should be tested to see which comes back with the highest accuracy for the text data in question. As many of these sentences are relatively short then it is unlikely that a high value of n would give significant accuracy increase but bigrams and trigrams could be worth exploring for a text like this.\n",
        "\n",
        "Another modification which could lead to accuracy improvement would be to consider lemmatization. Lemmatization is similar to stemming. It removes endings in text to return the base of a word known as its lemma. iTs implementation should improve accuracy by stopping words which share the same basic meaning being corrected to other words. For example if \"begin\" exists in our training set, and \"beginning\" does not, then we could say that the word beginning should not be changed to begin as it is a word with a similar meaning to begin.\n",
        "\n",
        "A modification which could improve accuracy and be useful alongside the lemmatization modification would be to check that a word which our algorithm does not recognise already exists as a valid word. This could be achieved by checking a dictionary for its existance. It could reduce the effects over lemmatization which can occur in the case where a spelling mistake (word which does not exist) could share the same stem as a correct word in the training data. \"Begining\" and \"Begning\" could share the same stem \"beg\" but should not be ignored as begning is not a valid word. The English wordnet.words() lexical database of semantic relations of words could be incorporated into an implementation of this modification.\n",
        "\n",
        "Named entity recognition could be used to improve accuracy by recognise names of entities such as \"Uncle Ben\". This could be achieved through the use of part of speech tagging in sentences. This could impolment the use of trees as studied in lectures. In the implementation of NER case should not be ignored and recognised named entities from the training data should be stored in a list for comparison.\n",
        "\n",
        "Through studying the results of the accuracy test in the previous task, it became clear that there were some cases where discrepancies between the given holbrook.txt correction and the correct() implementation correction. These discrepancies included:\n",
        "- numbers (eg. \"8\" -> \"48\")\n",
        "- days of the week (\"Tuesday\" -> \"Thursday\")\n",
        "- case (eg. \"No\" -> \"no\")\n",
        "These common discrepancies specific to the nature of the given text can be removed or reduced through application of some of the approaches discussed below.\n",
        "\n",
        "Some common discrepancies were seen in number correction where test numbers which did not appear in the train data were corrected to numbers which did appear in the train data. In order to imporve the accuracy of the algorithm in this area there are two possible approaches that could be used:\n",
        "1. If any number token is detected do not correct that token\n",
        "2. Add all numbers to the train dataset\n",
        "It would be more logical to implement the first option as it is more efficient and lends itself to simpler understanding.\n",
        "\n",
        "A list of days of the week and months of the year could be incorportaed to make up the base of the list of named entities. This modification would be likely to reduce the occurance of incorrect \"corrections\" such as \"Thursday\" to \"Tuesday\" where Tuesday exists in the train set but Thursday does not exist there in the original example.\n",
        "\n",
        "Another modification which could be emplyed to increase accuracy in the model would be to convert words which do not appear in the named entity list to lower case. This is an example of normalisation. An issue realted to this modifcation could arise if this were a grammar checker since capital letters are an important feature of English natural language grammar. However, as this algorithm is primarily concerned with the spelling of words ie. letter order: then capitalization can be ignored and remove discrepancies eg. \"Please\" corrected by algorithm to \"please\" which reduces accuracy score even though they share a common spelling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpGG7DbPIRHs"
      },
      "source": [
        "def get_candidates2(token):\n",
        "  tokens = []\n",
        "  for t in train:\n",
        "        tokens.extend(t[\"corrected\"])\n",
        "        #tokens.extend(t['original'])\n",
        "  token_set = set(tokens)\n",
        "  dmin = 100000\n",
        "  minDs = []\n",
        "  for t in token_set:\n",
        "      d = edit_distance(token, t)\n",
        "      if d == dmin:\n",
        "          minDs.append(t)\n",
        "      if d < dmin:\n",
        "          dmin = d\n",
        "          minDs = [t]\n",
        "  minDs.sort(reverse=True)\n",
        "  return minDs\n",
        "        \n",
        "# Test your code as follows\n",
        "#assert get_candidates2(\"minde\") == ['mine', 'mind']\n",
        "\n",
        "def correct2(sentence):\n",
        "    NER = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    tokens = []\n",
        "    for t in train:\n",
        "        tokens.extend(t[\"corrected\"])\n",
        "    token_set = set(tokens)\n",
        "    for word in sentence:\n",
        "        if word not in NER:\n",
        "          if word not in token_set:\n",
        "              candidates = get_candidates2(word)\n",
        "              highest_prob = 0\n",
        "              for candidate in candidates:\n",
        "                  probability = prob(word)\n",
        "                  if probability >= highest_prob:\n",
        "                      highest_prob = probability\n",
        "                      sentence = [candidate if (x == word) else x for x in sentence]\n",
        "    return sentence\n",
        "\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLzaC6D28sK9"
      },
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw6PzwWn7iEo"
      },
      "source": [
        "def accuracy2(test):\n",
        "    incorrect_count = 0\n",
        "    correct_count = 0\n",
        "    for sentence in test:\n",
        "      my_correction = correct2(sentence['original'])\n",
        "      for i in range(len(my_correction)):\n",
        "        if my_correction[i] != sentence['corrected'][i]:\n",
        "          incorrect_count += 1\n",
        "          #print(my_correction[i])\n",
        "          #print(sentence['corrected'][i])\n",
        "        else:\n",
        "          correct_count +=1\n",
        "    total = correct_count + incorrect_count\n",
        "    accuracy = correct_count/total\n",
        "    return accuracy\n",
        "\n",
        "print(accuracy(test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
